{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRUBK5K2EuCk"
      },
      "outputs": [],
      "source": [
        "!pip install gradio==3.44.4 transformers[sentencepiece] git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Function to clear GPU memory\n",
        "def clear_gpu_memory():\n",
        "    # Delete model and tensors if they are defined\n",
        "    global model, inputs, input_ids\n",
        "    if 'model' in globals():\n",
        "        del model\n",
        "    if 'inputs' in globals():\n",
        "        del inputs\n",
        "    if 'input_ids' in globals():\n",
        "        del input_ids\n",
        "\n",
        "    # Clear PyTorch cache\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Force Python's garbage collector to run\n",
        "    gc.collect()\n",
        "\n",
        "# Call the function to clear GPU memory\n",
        "clear_gpu_memory()"
      ],
      "metadata": {
        "id": "PMUw1tLKcVrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsOjziA3Dppt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import transformers\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "\"\"\"\n",
        "Helpers to support streaming generate output.\n",
        "Borrowed from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/callbacks.py\n",
        "\"\"\"\n",
        "\n",
        "import gc\n",
        "import traceback\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "class Stream(transformers.StoppingCriteria):\n",
        "    def __init__(self, callback_func=None):\n",
        "        self.callback_func = callback_func\n",
        "\n",
        "    def __call__(self, input_ids, scores) -> bool:\n",
        "        if self.callback_func is not None:\n",
        "            self.callback_func(input_ids[0])\n",
        "        return False\n",
        "\n",
        "class Iteratorize:\n",
        "    \"\"\"\n",
        "    Transforms a function that takes a callback\n",
        "    into a lazy iterator (generator).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, func, kwargs={}, callback=None):\n",
        "        self.mfunc = func\n",
        "        self.c_callback = callback\n",
        "        self.q = Queue()\n",
        "        self.sentinel = object()\n",
        "        self.kwargs = kwargs\n",
        "        self.stop_now = False\n",
        "\n",
        "        def _callback(val):\n",
        "            if self.stop_now:\n",
        "                raise ValueError\n",
        "            self.q.put(val)\n",
        "\n",
        "        def gentask():\n",
        "            try:\n",
        "                ret = self.mfunc(callback=_callback, **self.kwargs)\n",
        "            except ValueError:\n",
        "                pass\n",
        "            except:\n",
        "                traceback.print_exc()\n",
        "                pass\n",
        "\n",
        "            self.q.put(self.sentinel)\n",
        "            if self.c_callback:\n",
        "                self.c_callback(ret)\n",
        "\n",
        "        self.thread = Thread(target=gentask)\n",
        "        self.thread.start()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        obj = self.q.get(True, None)\n",
        "        if obj is self.sentinel:\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.stop_now = True\n",
        "\n",
        "\"\"\"\n",
        "A dedicated helper to manage templates and prompt building.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os.path as osp\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "class Prompter(object):\n",
        "    __slots__ = (\"template\", \"_verbose\")\n",
        "\n",
        "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
        "        self._verbose = verbose\n",
        "        template_name = \"alpaca\"\n",
        "        self.template = {\n",
        "            \"description\": \"Template used by Alpaca-LoRA.\",\n",
        "            \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
        "            \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
        "            \"response_split\": \"### Response:\"\n",
        "        }\n",
        "        if self._verbose:\n",
        "            print(\n",
        "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
        "            )\n",
        "\n",
        "    def generate_prompt(\n",
        "        self,\n",
        "        instruction: str,\n",
        "        input: Union[None, str] = None,\n",
        "        label: Union[None, str] = None,\n",
        "    ) -> str:\n",
        "        # returns the full prompt from instruction and optional input\n",
        "        # if a label (=response, =output) is provided, it's also appended.\n",
        "        if input:\n",
        "            res = self.template[\"prompt_input\"].format(\n",
        "                instruction=instruction, input=input\n",
        "            )\n",
        "        else:\n",
        "            res = self.template[\"prompt_no_input\"].format(\n",
        "                instruction=instruction\n",
        "            )\n",
        "        if label:\n",
        "            res = f\"{res}{label}\"\n",
        "        if self._verbose:\n",
        "            print(res)\n",
        "        return res\n",
        "\n",
        "    def get_response(self, output: str) -> str:\n",
        "        return output.split(self.template[\"response_split\"])[1].strip()\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:  # noqa: E722\n",
        "    pass\n",
        "\n",
        "\n",
        "base_model = 'openthaigpt/openthaigpt-1.0.0-beta-7b-chat-ckpt-hf'\n",
        "lora_weights = None\n",
        "load_8bit = False\n",
        "prompt_template = \"\"\n",
        "server_name = \"0.0.0.0\"\n",
        "share_gradio = True\n",
        "\n",
        "prompter = Prompter(prompt_template)\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "if device == \"cuda\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_8bit=load_8bit,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    if lora_weights:\n",
        "      model = PeftModel.from_pretrained(\n",
        "          model,\n",
        "          lora_weights,\n",
        "          torch_dtype=torch.float16,\n",
        "      )\n",
        "elif device == \"mps\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    if lora_weights:\n",
        "      model = PeftModel.from_pretrained(\n",
        "          model,\n",
        "          lora_weights,\n",
        "          device_map={\"\": device},\n",
        "          torch_dtype=torch.float16,\n",
        "      )\n",
        "else:\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
        "    )\n",
        "    if lora_weights:\n",
        "      model = PeftModel.from_pretrained(\n",
        "          model,\n",
        "          lora_weights,\n",
        "          device_map={\"\": device},\n",
        "      )\n",
        "\n",
        "# unwind broken decapoda-research config\n",
        "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
        "model.config.bos_token_id = 1\n",
        "model.config.eos_token_id = 2\n",
        "\n",
        "if not load_8bit:\n",
        "    model.half()  # seems to fix bugs for some users.\n",
        "\n",
        "model.eval()\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)\n",
        "\n",
        "def evaluate(\n",
        "    instruction,\n",
        "    input=None,\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=128,\n",
        "    stream_output=False,\n",
        "    repetition_penalty=1,\n",
        "    no_repeat_ngram=0,\n",
        "    **kwargs,\n",
        "):\n",
        "    prompt = prompter.generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    generate_params = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"generation_config\": generation_config,\n",
        "        \"return_dict_in_generate\": True,\n",
        "        \"output_scores\": True,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"early_stopping\": True,\n",
        "        \"repetition_penalty\":repetition_penalty,\n",
        "        \"no_repeat_ngram_size\":no_repeat_ngram\n",
        "    }\n",
        "\n",
        "\n",
        "    if stream_output:\n",
        "        # Stream the reply 1 token at a time.\n",
        "        # This is based on the trick of using 'stopping_criteria' to create an iterator,\n",
        "        # from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243.\n",
        "\n",
        "        def generate_with_callback(callback=None, **kwargs):\n",
        "            kwargs.setdefault(\n",
        "                \"stopping_criteria\", transformers.StoppingCriteriaList()\n",
        "            )\n",
        "            kwargs[\"stopping_criteria\"].append(\n",
        "                Stream(callback_func=callback)\n",
        "            )\n",
        "            with torch.no_grad():\n",
        "                model.generate(**kwargs)\n",
        "\n",
        "        def generate_with_streaming(**kwargs):\n",
        "            return Iteratorize(\n",
        "                generate_with_callback, kwargs, callback=None\n",
        "            )\n",
        "\n",
        "        with generate_with_streaming(**generate_params) as generator:\n",
        "            for output in generator:\n",
        "                # new_tokens = len(output) - len(input_ids[0])\n",
        "                decoded_output = tokenizer.decode(output)\n",
        "\n",
        "                if output[-1] in [tokenizer.eos_token_id]:\n",
        "                    break\n",
        "\n",
        "                yield prompter.get_response(decoded_output)\n",
        "        return  # early return for stream_output\n",
        "\n",
        "    # Without streaming\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            early_stopping=True,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            no_repeat_ngram_size=no_repeat_ngram\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    yield prompter.get_response(output)\n",
        "\n",
        "def clearText():\n",
        "    return [\"\",\"\",\"\"]\n",
        "\n",
        "def example1():\n",
        "    return [\"‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡πâ‡∏ß‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\",\"\"]\n",
        "\n",
        "def example2():\n",
        "    return [\"‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï ‡πÅ‡∏ö‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏±‡∏ß‡∏£‡πå\",\"‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏≤‡∏á‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ï‡πâ‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏≤‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ó‡∏∞‡πÄ‡∏•‡∏≠‡∏±‡∏ô‡∏î‡∏≤‡∏°‡∏±‡∏ô ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏≤‡∏á‡∏ó‡∏¥‡∏®‡πÄ‡∏´‡∏ô‡∏∑‡∏≠ ‡∏Ñ‡∏∑‡∏≠ ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏±‡∏á‡∏á‡∏≤ ‡∏ó‡∏≤‡∏á‡∏ó‡∏¥‡∏®‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å ‡∏Ñ‡∏∑‡∏≠ ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏±‡∏á‡∏á‡∏≤ ‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏Å‡∏≤‡∏∞‡∏•‡πâ‡∏≠‡∏°‡∏£‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏°‡∏´‡∏≤‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏≠‡∏¥‡∏ô‡πÄ‡∏î‡∏µ‡∏¢ ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÄ‡∏Å‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡∏≤‡∏ì‡∏≤‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï‡∏ó‡∏≤‡∏á‡∏ó‡∏¥‡∏®‡πÉ‡∏ï‡πâ‡πÅ‡∏•‡∏∞‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å ‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ó‡∏≤‡∏á‡πÄ‡∏£‡∏∑‡∏≠‡πÅ‡∏•‡πâ‡∏ß ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÇ‡∏î‡∏¢‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏≤‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏±‡∏á‡∏á‡∏≤ ‡πÇ‡∏î‡∏¢‡∏Ç‡πâ‡∏≤‡∏°‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏™‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏Ñ‡∏π‡πà‡∏Ç‡∏ô‡∏≤‡∏ô ‡∏Ñ‡∏∑‡∏≠ ‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏ó‡πâ‡∏≤‡∏ß‡πÄ‡∏ó‡∏û‡∏Å‡∏£‡∏∞‡∏©‡∏±‡∏ï‡∏£‡∏µ‡πÅ‡∏•‡∏∞‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏ó‡πâ‡∏≤‡∏ß‡∏®‡∏£‡∏µ‡∏™‡∏∏‡∏ô‡∏ó‡∏£ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏ï‡∏±‡∏ß‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î ‡πÅ‡∏•‡∏∞‡∏ó‡∏≤‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏ó‡πà‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏¢‡∏≤‡∏ô‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ‡∏ó‡πà‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏¢‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏≤‡∏á‡∏ó‡∏¥‡∏®‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏ï‡∏Å‡πÄ‡∏â‡∏µ‡∏¢‡∏á‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏≤‡∏∞\"]\n",
        "\n",
        "def example3():\n",
        "    return [\"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö \\\"‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏Å‡∏à‡∏¥‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏£‡∏µ‡πà\\\"\",\"\"]\n",
        "\n",
        "def example4():\n",
        "    return [\"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î\",\"python pandas csv export\"]\n",
        "\n",
        "def example5():\n",
        "    return [\"x+30=100 x=?\",\"\"]\n",
        "\n",
        "def example6():\n",
        "    return [\"‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©\",\"‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£ ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡πÅ‡∏•‡∏∞‡∏ô‡∏Ñ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡∏Ç‡∏ô‡∏™‡πà‡∏á ‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ ‡∏Å‡∏≤‡∏£‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå ‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®\"]\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # üáπüá≠ OpenThaiGPT 1.0.0-beta\n",
        "        üáπüá≠ OpenThaiGPT Version 1.0.0-beta is a Thai language 7B-parameter LLaMA v2 Chat model finetuned to follow Thai translated instructions and extend 24,554 Thai words vocabularies for turbo speed. For more information, please visit [the project's website](https://openthaigpt.aieat.or.th/) | [Github](https://github.com/OpenThaiGPT/openthaigpt).\n",
        "\n",
        "        ## Examples\n",
        "        \"\"\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        example1_button = gr.Button(value=\"‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡πâ‡∏ß‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\")\n",
        "        example2_button = gr.Button(value=\"‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï ‡πÅ‡∏ö‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏±‡∏ß‡∏£‡πå\")\n",
        "        example3_button = gr.Button(value=\"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°\")\n",
        "        example4_button = gr.Button(value=\"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î\")\n",
        "        example5_button = gr.Button(value=\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\")\n",
        "        example6_button = gr.Button(value=\"‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤\")\n",
        "\n",
        "    instbox = gr.components.Textbox(\n",
        "            lines=2,\n",
        "            label=\"Instruction\",\n",
        "            placeholder=\"‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á\",\n",
        "            value=\"‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡πâ‡∏ß‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\"\n",
        "        )\n",
        "    inputbox = gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\")\n",
        "    streambox = gr.components.Checkbox(label=\"Stream output\", value=True)\n",
        "    button = gr.Button(value=\"Generate\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        cancel = gr.Button(value=\"Stop / Cancel\")\n",
        "        clear = gr.Button(value=\"Clear\")\n",
        "\n",
        "    outputbox = gr.inputs.Textbox(\n",
        "            lines=5,\n",
        "            label=\"Output\",\n",
        "        )\n",
        "\n",
        "    with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "        tempbox = gr.components.Slider(\n",
        "            minimum=0, maximum=1, value=0.1, info=\"‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥: ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö ‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡∏™‡∏π‡∏á ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡∏ï‡πà‡∏≥ ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∞‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\", label=\"Temperature\"\n",
        "        )\n",
        "        toppbox = gr.components.Slider(\n",
        "            minimum=0, maximum=1, value=0.75, info=\"nucleus sampling: ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ñ‡∏∂‡∏á p%\", label=\"Top p\"\n",
        "        )\n",
        "        topkbox = gr.components.Slider(\n",
        "            minimum=0, maximum=100, step=1, value=40, info=\"top-k sampling: ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å k ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∂‡∏á‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô k ‡∏Ñ‡∏≥‡∏ô‡∏±‡πâ‡∏ô\", label=\"Top k\"\n",
        "        )\n",
        "        beambox = gr.components.Slider(\n",
        "            minimum=1, maximum=4, step=1, value=1, info=\"beam: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Beam ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\", label=\"Beams\"\n",
        "        )\n",
        "        maxtokenbox = gr.components.Slider(\n",
        "            minimum=1, maximum=4096, step=1, value=512, info=\"max_token: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\", label=\"Max tokens\"\n",
        "        )\n",
        "        repetition_penalty_box = gr.components.Slider(\n",
        "            minimum=1, maximum=1.99, step=0.01, value=1.2, info=\"repetition_penalty: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡πÇ‡∏ó‡∏©‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥ 1=‡πÑ‡∏°‡πà‡∏•‡∏á‡πÇ‡∏ó‡∏© 1.99=‡∏•‡∏á‡πÇ‡∏ó‡∏©‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\", label=\"Repetition Penalty\"\n",
        "        )\n",
        "        no_repeat_ngram_box = gr.components.Slider(\n",
        "            minimum=0, maximum=30, step=0, value=4, info=\"no_repeat_ngram: ‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£\", label=\"No Repeat N-GRAM\"\n",
        "        )\n",
        "\n",
        "    button_click_event = button.click(fn=evaluate, inputs=[instbox, inputbox, tempbox, toppbox, topkbox, beambox, maxtokenbox, streambox, repetition_penalty_box, no_repeat_ngram_box], outputs=outputbox)\n",
        "    cancel.click(fn=None, inputs=None, outputs=None, cancels=[button_click_event])\n",
        "    clear.click(fn=clearText, outputs=[instbox, inputbox, outputbox])\n",
        "\n",
        "    example1_button.click(fn=example1, outputs=[instbox, inputbox])\n",
        "    example2_button.click(fn=example2, outputs=[instbox, inputbox])\n",
        "    example3_button.click(fn=example3, outputs=[instbox, inputbox])\n",
        "    example4_button.click(fn=example4, outputs=[instbox, inputbox])\n",
        "    example5_button.click(fn=example5, outputs=[instbox, inputbox])\n",
        "    example6_button.click(fn=example6, outputs=[instbox, inputbox])\n",
        "\n",
        "demo.queue().launch(server_name=\"0.0.0.0\", share=share_gradio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0bmc4gQa6XS"
      },
      "source": [
        "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ Click Link: https:// xxxx .gradio.live ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏é‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î New Tab ‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≠"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
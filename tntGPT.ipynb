{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRUBK5K2EuCk"
      },
      "outputs": [],
      "source": [
        "!pip install gradio==3.44.4 transformers[sentencepiece] git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Function to clear GPU memory\n",
        "def clear_gpu_memory():\n",
        "    # Delete model and tensors if they are defined\n",
        "    global model, inputs, input_ids\n",
        "    if 'model' in globals():\n",
        "        del model\n",
        "    if 'inputs' in globals():\n",
        "        del inputs\n",
        "    if 'input_ids' in globals():\n",
        "        del input_ids\n",
        "\n",
        "    # Clear PyTorch cache\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Force Python's garbage collector to run\n",
        "    gc.collect()\n",
        "\n",
        "# Call the function to clear GPU memory\n",
        "clear_gpu_memory()"
      ],
      "metadata": {
        "id": "PMUw1tLKcVrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsOjziA3Dppt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import transformers\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "\"\"\"\n",
        "Helpers to support streaming generate output.\n",
        "Borrowed from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/callbacks.py\n",
        "\"\"\"\n",
        "\n",
        "import gc\n",
        "import traceback\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "class Stream(transformers.StoppingCriteria):\n",
        "    def __init__(self, callback_func=None):\n",
        "        self.callback_func = callback_func\n",
        "\n",
        "    def __call__(self, input_ids, scores) -> bool:\n",
        "        if self.callback_func is not None:\n",
        "            self.callback_func(input_ids[0])\n",
        "        return False\n",
        "\n",
        "class Iteratorize:\n",
        "    \"\"\"\n",
        "    Transforms a function that takes a callback\n",
        "    into a lazy iterator (generator).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, func, kwargs={}, callback=None):\n",
        "        self.mfunc = func\n",
        "        self.c_callback = callback\n",
        "        self.q = Queue()\n",
        "        self.sentinel = object()\n",
        "        self.kwargs = kwargs\n",
        "        self.stop_now = False\n",
        "\n",
        "        def _callback(val):\n",
        "            if self.stop_now:\n",
        "                raise ValueError\n",
        "            self.q.put(val)\n",
        "\n",
        "        def gentask():\n",
        "            try:\n",
        "                ret = self.mfunc(callback=_callback, **self.kwargs)\n",
        "            except ValueError:\n",
        "                pass\n",
        "            except:\n",
        "                traceback.print_exc()\n",
        "                pass\n",
        "\n",
        "            self.q.put(self.sentinel)\n",
        "            if self.c_callback:\n",
        "                self.c_callback(ret)\n",
        "\n",
        "        self.thread = Thread(target=gentask)\n",
        "        self.thread.start()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        obj = self.q.get(True, None)\n",
        "        if obj is self.sentinel:\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.stop_now = True\n",
        "\n",
        "\"\"\"\n",
        "A dedicated helper to manage templates and prompt building.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os.path as osp\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "class Prompter(object):\n",
        "    __slots__ = (\"template\", \"_verbose\")\n",
        "\n",
        "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
        "        self._verbose = verbose\n",
        "        template_name = \"alpaca\"\n",
        "        self.template = {\n",
        "            \"description\": \"Template used by Alpaca-LoRA.\",\n",
        "            \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
        "            \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
        "            \"response_split\": \"### Response:\"\n",
        "        }\n",
        "        if self._verbose:\n",
        "            print(\n",
        "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
        "            )\n",
        "\n",
        "    def generate_prompt(\n",
        "        self,\n",
        "        instruction: str,\n",
        "        input: Union[None, str] = None,\n",
        "        label: Union[None, str] = None,\n",
        "    ) -> str:\n",
        "        # returns the full prompt from instruction and optional input\n",
        "        # if a label (=response, =output) is provided, it's also appended.\n",
        "        if input:\n",
        "            res = self.template[\"prompt_input\"].format(\n",
        "                instruction=instruction, input=input\n",
        "            )\n",
        "        else:\n",
        "            res = self.template[\"prompt_no_input\"].format(\n",
        "                instruction=instruction\n",
        "            )\n",
        "        if label:\n",
        "            res = f\"{res}{label}\"\n",
        "        if self._verbose:\n",
        "            print(res)\n",
        "        return res\n",
        "\n",
        "    def get_response(self, output: str) -> str:\n",
        "        return output.split(self.template[\"response_split\"])[1].strip()\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:  # noqa: E722\n",
        "    pass\n",
        "\n",
        "\n",
        "base_model = 'openthaigpt/openthaigpt-1.0.0-beta-7b-chat-ckpt-hf'\n",
        "lora_weights = None\n",
        "load_8bit = False\n",
        "prompt_template = \"\"\n",
        "server_name = \"0.0.0.0\"\n",
        "share_gradio = True\n",
        "\n",
        "prompter = Prompter(prompt_template)\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "if device == \"cuda\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_8bit=load_8bit,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    if lora_weights:\n",
        "      model = PeftModel.from_pretrained(\n",
        "          model,\n",
        "          lora_weights,\n",
        "          torch_dtype=torch.float16,\n",
        "      )\n",
        "elif device == \"mps\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    if lora_weights:\n",
        "      model = PeftModel.from_pretrained(\n",
        "          model,\n",
        "          lora_weights,\n",
        "          device_map={\"\": device},\n",
        "          torch_dtype=torch.float16,\n",
        "      )\n",
        "else:\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
        "    )\n",
        "    if lora_weights:\n",
        "      model = PeftModel.from_pretrained(\n",
        "          model,\n",
        "          lora_weights,\n",
        "          device_map={\"\": device},\n",
        "      )\n",
        "\n",
        "# unwind broken decapoda-research config\n",
        "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
        "model.config.bos_token_id = 1\n",
        "model.config.eos_token_id = 2\n",
        "\n",
        "if not load_8bit:\n",
        "    model.half()  # seems to fix bugs for some users.\n",
        "\n",
        "model.eval()\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)\n",
        "\n",
        "def evaluate(\n",
        "    instruction,\n",
        "    input=None,\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=128,\n",
        "    stream_output=False,\n",
        "    repetition_penalty=1,\n",
        "    no_repeat_ngram=0,\n",
        "    **kwargs,\n",
        "):\n",
        "    prompt = prompter.generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    generate_params = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"generation_config\": generation_config,\n",
        "        \"return_dict_in_generate\": True,\n",
        "        \"output_scores\": True,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"early_stopping\": True,\n",
        "        \"repetition_penalty\":repetition_penalty,\n",
        "        \"no_repeat_ngram_size\":no_repeat_ngram\n",
        "    }\n",
        "\n",
        "\n",
        "    if stream_output:\n",
        "        # Stream the reply 1 token at a time.\n",
        "        # This is based on the trick of using 'stopping_criteria' to create an iterator,\n",
        "        # from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243.\n",
        "\n",
        "        def generate_with_callback(callback=None, **kwargs):\n",
        "            kwargs.setdefault(\n",
        "                \"stopping_criteria\", transformers.StoppingCriteriaList()\n",
        "            )\n",
        "            kwargs[\"stopping_criteria\"].append(\n",
        "                Stream(callback_func=callback)\n",
        "            )\n",
        "            with torch.no_grad():\n",
        "                model.generate(**kwargs)\n",
        "\n",
        "        def generate_with_streaming(**kwargs):\n",
        "            return Iteratorize(\n",
        "                generate_with_callback, kwargs, callback=None\n",
        "            )\n",
        "\n",
        "        with generate_with_streaming(**generate_params) as generator:\n",
        "            for output in generator:\n",
        "                # new_tokens = len(output) - len(input_ids[0])\n",
        "                decoded_output = tokenizer.decode(output)\n",
        "\n",
        "                if output[-1] in [tokenizer.eos_token_id]:\n",
        "                    break\n",
        "\n",
        "                yield prompter.get_response(decoded_output)\n",
        "        return  # early return for stream_output\n",
        "\n",
        "    # Without streaming\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            early_stopping=True,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            no_repeat_ngram_size=no_repeat_ngram\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    yield prompter.get_response(output)\n",
        "\n",
        "def clearText():\n",
        "    return [\"\",\"\",\"\"]\n",
        "\n",
        "def example1():\n",
        "    return [\"ลดความอ้วนต้องทำอย่างไร\",\"\"]\n",
        "\n",
        "def example2():\n",
        "    return [\"วางแผนเที่ยวในภูเก็ต แบบบริษัททัวร์\",\"ภูเก็ต เป็นจังหวัดหนึ่งทางภาคใต้ของประเทศไทย และเป็นเกาะขนาดใหญ่ที่สุดในประเทศไทย อยู่ในทะเลอันดามัน จังหวัดที่ใกล้เคียงทางทิศเหนือ คือ จังหวัดพังงา ทางทิศตะวันออก คือ จังหวัดพังงา ทั้งเกาะล้อมรอบด้วยมหาสมุทรอินเดีย และยังมีเกาะที่อยู่ในอาณาเขตของจังหวัดภูเก็ตทางทิศใต้และตะวันออก การเดินทางเข้าสู่ภูเก็ตนอกจากทางเรือแล้ว สามารถเดินทางโดยรถยนต์ซึ่งมีเพียงเส้นทางเดียวผ่านทางจังหวัดพังงา โดยข้ามสะพานสารสินและสะพานคู่ขนาน คือ สะพานท้าวเทพกระษัตรีและสะพานท้าวศรีสุนทร เพื่อเข้าสู่ตัวจังหวัด และทางอากาศโดยมีท่าอากาศยานนานาชาติภูเก็ตรองรับ ท่าอากาศยานนี้ตั้งอยู่ทางทิศตะวันตกเฉียงเหนือของเกาะ\"]\n",
        "\n",
        "def example3():\n",
        "    return [\"เขียนบทความเกี่ยวกับ \\\"ประโยชน์ของโกจิเบอร์รี่\\\"\",\"\"]\n",
        "\n",
        "def example4():\n",
        "    return [\"เขียนโค้ด\",\"python pandas csv export\"]\n",
        "\n",
        "def example5():\n",
        "    return [\"x+30=100 x=?\",\"\"]\n",
        "\n",
        "def example6():\n",
        "    return [\"แปลภาษาไทยเป็นอังกฤษ\",\"กรุงเทพมหานคร เป็นเมืองหลวงและนครที่มีประชากรมากที่สุดของประเทศไทย เป็นศูนย์กลางการปกครอง การศึกษา การคมนาคมขนส่ง การเงินการธนาคาร การพาณิชย์ การสื่อสาร และความเจริญของประเทศ\"]\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # 🇹🇭 OpenThaiGPT 1.0.0-beta\n",
        "        🇹🇭 OpenThaiGPT Version 1.0.0-beta is a Thai language 7B-parameter LLaMA v2 Chat model finetuned to follow Thai translated instructions and extend 24,554 Thai words vocabularies for turbo speed. For more information, please visit [the project's website](https://openthaigpt.aieat.or.th/) | [Github](https://github.com/OpenThaiGPT/openthaigpt).\n",
        "\n",
        "        ## Examples\n",
        "        \"\"\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        example1_button = gr.Button(value=\"ลดความอ้วนต้องทำอย่างไร\")\n",
        "        example2_button = gr.Button(value=\"วางแผนเที่ยวในภูเก็ต แบบบริษัททัวร์\")\n",
        "        example3_button = gr.Button(value=\"เขียนบทความ\")\n",
        "        example4_button = gr.Button(value=\"เขียนโค้ด\")\n",
        "        example5_button = gr.Button(value=\"คำนวณคณิตศาสตร์\")\n",
        "        example6_button = gr.Button(value=\"แปลภาษา\")\n",
        "\n",
        "    instbox = gr.components.Textbox(\n",
        "            lines=2,\n",
        "            label=\"Instruction\",\n",
        "            placeholder=\"คำสั่ง\",\n",
        "            value=\"ลดความอ้วนต้องทำอย่างไร\"\n",
        "        )\n",
        "    inputbox = gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"คำถาม (ไม่จำเป็น)\")\n",
        "    streambox = gr.components.Checkbox(label=\"Stream output\", value=True)\n",
        "    button = gr.Button(value=\"Generate\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        cancel = gr.Button(value=\"Stop / Cancel\")\n",
        "        clear = gr.Button(value=\"Clear\")\n",
        "\n",
        "    outputbox = gr.inputs.Textbox(\n",
        "            lines=5,\n",
        "            label=\"Output\",\n",
        "        )\n",
        "\n",
        "    with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "        tempbox = gr.components.Slider(\n",
        "            minimum=0, maximum=1, value=0.1, info=\"อุณหภูมิ: พารามิเตอร์นี้ใช้ควบคุมความเสี่ยงในการสร้างข้อความของระบบ ถ้าตั้งค่าไว้สูง การสร้างข้อความจะเป็นลักษณะที่หลากหลายมากขึ้น ถ้าตั้งค่าไว้ต่ำ การสร้างข้อความจะมีลักษณะที่มีโครงสร้างแน่นอนมากขึ้น\", label=\"Temperature\"\n",
        "        )\n",
        "        toppbox = gr.components.Slider(\n",
        "            minimum=0, maximum=1, value=0.75, info=\"nucleus sampling: พารามิเตอร์นี้ใช้เป็นวิธีการสุ่มตัวเลือกจากคำที่อาจจะถูกเลือกถัดไป ระบบจะสุ่มเลือกจากกลุ่มคำที่มีความน่าจะเป็นรวมกันสูงสุดถึง p%\", label=\"Top p\"\n",
        "        )\n",
        "        topkbox = gr.components.Slider(\n",
        "            minimum=0, maximum=100, step=1, value=40, info=\"top-k sampling: พารามิเตอร์นี้ใช้เลือก k คำที่มีความน่าจะเป็นสูงสุดสำหรับคำถัดไป แล้วจึงสุ่มเลือกหนึ่งใน k คำนั้น\", label=\"Top k\"\n",
        "        )\n",
        "        beambox = gr.components.Slider(\n",
        "            minimum=1, maximum=4, step=1, value=1, info=\"beam: จำนวนวิธีการสร้างข้อความโดยใช้คำหลายๆ ทางเลือกที่น่าจะเป็นที่สุดในแต่ละขั้นตอน การตั้งค่า Beam ที่สูงขึ้นจะทำให้สามารถสำรวจคำหลายทางเลือกมากขึ้น แต่จะเพิ่มการคำนวณและอาจจะไม่ทำให้ผลลัพธ์ดีขึ้นทุกครั้ง\", label=\"Beams\"\n",
        "        )\n",
        "        maxtokenbox = gr.components.Slider(\n",
        "            minimum=1, maximum=4096, step=1, value=512, info=\"max_token: ความยาวของคำตอบ\", label=\"Max tokens\"\n",
        "        )\n",
        "        repetition_penalty_box = gr.components.Slider(\n",
        "            minimum=1, maximum=1.99, step=0.01, value=1.2, info=\"repetition_penalty: ความรุนแรงในการลงโทษเมื่อตอบข้อความซ้ำ 1=ไม่ลงโทษ 1.99=ลงโทษสูงสุด\", label=\"Repetition Penalty\"\n",
        "        )\n",
        "        no_repeat_ngram_box = gr.components.Slider(\n",
        "            minimum=0, maximum=30, step=0, value=4, info=\"no_repeat_ngram: การป้องกันการตอบข้อความซ้ำตามจำนวนตัวอักษร\", label=\"No Repeat N-GRAM\"\n",
        "        )\n",
        "\n",
        "    button_click_event = button.click(fn=evaluate, inputs=[instbox, inputbox, tempbox, toppbox, topkbox, beambox, maxtokenbox, streambox, repetition_penalty_box, no_repeat_ngram_box], outputs=outputbox)\n",
        "    cancel.click(fn=None, inputs=None, outputs=None, cancels=[button_click_event])\n",
        "    clear.click(fn=clearText, outputs=[instbox, inputbox, outputbox])\n",
        "\n",
        "    example1_button.click(fn=example1, outputs=[instbox, inputbox])\n",
        "    example2_button.click(fn=example2, outputs=[instbox, inputbox])\n",
        "    example3_button.click(fn=example3, outputs=[instbox, inputbox])\n",
        "    example4_button.click(fn=example4, outputs=[instbox, inputbox])\n",
        "    example5_button.click(fn=example5, outputs=[instbox, inputbox])\n",
        "    example6_button.click(fn=example6, outputs=[instbox, inputbox])\n",
        "\n",
        "demo.queue().launch(server_name=\"0.0.0.0\", share=share_gradio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0bmc4gQa6XS"
      },
      "source": [
        "สามารถ Click Link: https:// xxxx .gradio.live ที่ปรากฎด้านบน เพื่อเปิด New Tab ใช้แบบเต็มจอ"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}